import wikipedia
import nltk
from nltk.corpus import wordnet as wn
#print(wikipedia.search('Vincent'))

'''
person=input("PLEASE ENTER A PERSON TO SEARCH")
if (wikipedia.search(person))== None:
    print("NO WIKIPEDIA PAGE EXISTS ON THAT PERSON")
'''
summary=(wikipedia.summary("Vincent Van Gogh")) #this returns a list of characters
words= summary.split()#returns a list split based on white space

summary=(wikipedia.summary("walter white")) #this returns a list of characters
words= summary.split()#returns a list split based on white space

adjectives_or_nouns=[]
for word in words:
    if((len(wn.synsets(word)) > 0) and (wn.synsets(word)[0].pos()=='a' or wn.synsets(word)[0].pos()=='n')):
        #adjectives.append(wn.synsets(word))
        adjectives_or_nouns.append(word)
print(len(adjectives_or_nouns))

print(len(set(adjectives_or_nouns)))
print((adjectives_or_nouns))


exampleList=['hot','coffee','bitter','caffeine']
print(wn.synsets(exampleList[1]))
for w in exampleList:
    syn=(wn.synsets(w)[0])
    print("TAG: "+ syn.pos())
'''

print(len(words))

pos_all=dict()

print(words[82])
print(wn.synsets(words[82]))
print(wn.synsets(words[82])[0])
'''
'''

for w in words:
    pos_l=set()
    for tmp in wn.synsets(w):
        if tmp.name().split('.')[0]==w:
            pos_l.add(tmp.pos())
        pos_all[w]=pos_l
print(pos_all)
'''
#sentences= nltk.sent_tokenize(summary)
#print(sentences)
'''
#eliminate most common words... as words with the highest count are insignificant
word_freq={}
for word in words:
    if word in word_freq:
        word_freq[word]+=1
    else:
        word_freq[word]=1

#print(word_freq)
unique=[]
for word, freq in word_freq.items():
    if freq > 1:
        print(word)
    else:
        unique.append(word)
        
